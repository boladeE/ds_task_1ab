{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c43964-4666-4d93-b2bd-5d8b786f8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381099df-8d26-4b9a-99dd-9552baa1dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to clean images\n",
    "\n",
    "data_dir = 'data' \n",
    "image_exts = ['jpeg','jpg', 'bmp', 'png']\n",
    "for image_class in os.listdir(data_dir): \n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try: \n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_exts: \n",
    "                print('Image not in ext list {}'.format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e: \n",
    "            print('Issue with image {}'.format(image_path))\n",
    "            # os.remove(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e31ffda9-bf0a-4968-b76f-284dd6b549d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1116 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using image_dataset_from_directory\n",
    "data = tf.keras.utils.image_dataset_from_directory('data')\n",
    "data = data.map(lambda x,y: (x/255, y))\n",
    "train_size = int(len(data)*.7)\n",
    "val_size = int(len(data)*.2)\n",
    "test_size = int(len(data)*.1)\n",
    "\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e6d649-75e6-4d9b-b543-9b50a8b7cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab67982-0021-436a-8a5b-d9289b77220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 14400)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               3686656   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,698,938\n",
      "Trainable params: 3,698,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7baf53c4-f4c2-4fa8-a070-dfd53d678130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 14s 449ms/step - loss: 0.5250 - accuracy: 0.8337 - val_loss: 0.4017 - val_accuracy: 0.8909\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 13s 435ms/step - loss: 0.5058 - accuracy: 0.8382 - val_loss: 0.4064 - val_accuracy: 0.9045\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 14s 488ms/step - loss: 0.4465 - accuracy: 0.8605 - val_loss: 0.2959 - val_accuracy: 0.9545\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 15s 504ms/step - loss: 0.4052 - accuracy: 0.8772 - val_loss: 0.3098 - val_accuracy: 0.9273\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 15s 494ms/step - loss: 0.3698 - accuracy: 0.8817 - val_loss: 0.2895 - val_accuracy: 0.9500\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 14s 484ms/step - loss: 0.3593 - accuracy: 0.8839 - val_loss: 0.2551 - val_accuracy: 0.9636\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 14s 471ms/step - loss: 0.3107 - accuracy: 0.9196 - val_loss: 0.2422 - val_accuracy: 0.9545\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 14s 480ms/step - loss: 0.2661 - accuracy: 0.9241 - val_loss: 0.2011 - val_accuracy: 0.9727\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 15s 488ms/step - loss: 0.2637 - accuracy: 0.9230 - val_loss: 0.2095 - val_accuracy: 0.9727\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 15s 492ms/step - loss: 0.2749 - accuracy: 0.9107 - val_loss: 0.1830 - val_accuracy: 0.9500\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 15s 495ms/step - loss: 0.2646 - accuracy: 0.9230 - val_loss: 0.1646 - val_accuracy: 0.9636\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 15s 493ms/step - loss: 0.2538 - accuracy: 0.9342 - val_loss: 0.1808 - val_accuracy: 0.9636\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 14s 491ms/step - loss: 0.2175 - accuracy: 0.9431 - val_loss: 0.1536 - val_accuracy: 0.9727\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 15s 492ms/step - loss: 0.2200 - accuracy: 0.9297 - val_loss: 0.1272 - val_accuracy: 0.9727\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 15s 496ms/step - loss: 0.1917 - accuracy: 0.9554 - val_loss: 0.1386 - val_accuracy: 0.9727\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 15s 511ms/step - loss: 0.1837 - accuracy: 0.9464 - val_loss: 0.1131 - val_accuracy: 0.9727\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 18s 591ms/step - loss: 0.1995 - accuracy: 0.9464 - val_loss: 0.1087 - val_accuracy: 0.9773\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 16s 543ms/step - loss: 0.1825 - accuracy: 0.9509 - val_loss: 0.1319 - val_accuracy: 0.9682\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 15s 504ms/step - loss: 0.1481 - accuracy: 0.9665 - val_loss: 0.1194 - val_accuracy: 0.9773\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 15s 515ms/step - loss: 0.1490 - accuracy: 0.9554 - val_loss: 0.1076 - val_accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b25b7f70-7eec-42e2-8c8e-9aaef740e60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 6s 103ms/step - loss: 0.1011 - accuracy: 0.9864\n",
      "Validation Loss: 0.1010923907160759\n",
      "Validation Accuracy: 0.9863636493682861\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5790919-1253-49e1-b9bb-4cc27d1cac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"product_classification_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25acae8a-cc90-4178-85e5-77a801527142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 333ms/step\n",
      "[9]\n",
      "['6 RIBBONS RUSTIC CHARM', 'ALARM CLOCK BAKELIKE RED', 'CHOCOLATE HOT WATER BOTTLE', 'JUMBO STORAGE BAG SUKI', 'LUNCH BAG PINK POLKADOT', 'LUNCH BAG WOODLAND', 'REGENCY CAKESTAND 3 TIER', 'RETROSPOT TEA SET CERAMIC 11 PC', 'REX CASHCARRY JUMBO SHOPPER', 'SPOTTY BUNTING']\n",
      "Predicted Label: SPOTTY BUNTING\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model(\"product_classification_cnn.h5\")\n",
    "\n",
    "# Load and preprocess the new image\n",
    "img_path = 'data/SPOTTY BUNTING/71FLL7rTUJL.jpg'\n",
    "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array /= 255.0  # Normalize pixel values\n",
    "\n",
    "# Make a prediction\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(predicted_class)\n",
    "# Map the predicted class index to the label\n",
    "class_labels = dataset.class_names  # Get class names from the dataset\n",
    "print(class_labels)\n",
    "predicted_label = class_labels[predicted_class[0]]\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee817a-71fd-459f-a4a2-13f0cb21258d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
